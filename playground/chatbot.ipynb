{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "febbcbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28c4ff25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\sagnik\\AppData\\Local\\Temp\\ipykernel_27016\\63906354.py:14: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  vector_db = Chroma.from_documents(final_documents,ollama_embeddings, persist_directory = 'stored_data\\chroma_db')\n"
     ]
    }
   ],
   "source": [
    "#### Storing and retrieving from the databases, and performing a similarity search\n",
    "ollama_embeddings=(\n",
    "    OllamaEmbeddings(model=\"gemma:2b\")  ##by default it ues llama2. gemma:2b is downloaded in local pc\n",
    ")\n",
    "\n",
    "# Loading the research paper\n",
    "\n",
    "pdf_loader = PyPDFLoader('ip_data/attention-is-all-you-need.pdf')\n",
    "docs = pdf_loader.load()\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "final_documents=text_splitter.split_documents(docs)\n",
    "\n",
    "# Storing the documents in the chroma db\n",
    "vector_db = Chroma.from_documents(final_documents,ollama_embeddings, persist_directory = 'stored_data\\chroma_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c96ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='b91e0727-de82-4b46-a592-ba21038e75f1', metadata={'page': 1, 'source': 'ip_data/attention-is-all-you-need.pdf'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].'), Document(id='001cf9c7-96b4-4e90-b81f-41ff85cfc7b9', metadata={'source': 'ip_data/attention-is-all-you-need.pdf', 'page': 6}, page_content='between any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.'), Document(id='aa308ad9-6b14-42b5-a03b-4a9ae68ac6b3', metadata={'page': 3, 'source': 'ip_data/attention-is-all-you-need.pdf'}, page_content='queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.'), Document(id='f065f064-4507-4b6d-8583-bb65bee9c9c2', metadata={'page': 4, 'source': 'ip_data/attention-is-all-you-need.pdf'}, page_content='3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].')]\n"
     ]
    }
   ],
   "source": [
    "# Performing a simimarity search from the chroma db store : \n",
    "query = \"How exactly the self-attention mechanism works?\"\n",
    "docs_from_query = vector_db.similarity_search(query)\n",
    "print(docs_from_query)\n",
    "\n",
    "# How to get the docs in a better format?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac071a06",
   "metadata": {},
   "source": [
    "### Building a chatbot through Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af27f039",
   "metadata": {},
   "source": [
    "### Building a chatbot through OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3bb6f",
   "metadata": {},
   "source": [
    "### Building a chatbot through Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6e317",
   "metadata": {},
   "source": [
    "### Building a chatbot through GroqAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73dc1f",
   "metadata": {},
   "source": [
    "#### Storing things in a single database - select your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdf9f9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82ecef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = docs[0].page_content.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d06e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
